# -*- coding: utf-8 -*-
"""Life Expectancy(WHO).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/176NPqhWgKlKnZ8MU_Uqb5vtNOIW099Z-
"""

import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import scipy.stats
import openpyxl

life_xp = pd.read_csv('Life Expectancy Data.csv')

"""# **DATA OVERVIEW**"""

# Display information about the DataFrame life_xp
life_xp.info()

life_xp.describe() # Generate descriptive statistics of the DataFrame life_xp

life_xp.head() # Display the first few rows of the DataFrame life_xp

life_xp.columns = life_xp.columns.str.strip() # Remove leading and trailing whitespace from column names in the DataFrame life_xp

for col in life_xp.columns:
    print(str(life_xp[col].value_counts()))
    print("-----------------------------------------")

missing_values = life_xp.isnull().sum() # count of missing values
null_values = life_xp.isna().sum() # count of null values

print("Missing values count for each column:")
print(missing_values)

print("Null value count for each column:")
print(null_values)

# Calculate the percentage of null values in each column of the DataFrame life_xp

null_percentages = life_xp.isnull().mean(axis=0) * 100

plt.figure(figsize=(12, 6))
plt.plot(null_percentages, marker='o', color='blue', linestyle='-')
plt.title('Percentage of Null Values in Each Column')
plt.xlabel('Columns')
plt.ylabel('Null Value Percentage (%)')
plt.xticks(rotation=90)
plt.grid(axis='y', linestyle='--', alpha=0.7)
for x, y in enumerate(null_percentages):
    plt.text(x, y + 0.5, f'{y:.1f}%', ha='center')
plt.tight_layout()
plt.show()

for col in life_xp.columns:  # Iterate over each column in the DataFrame life_xp
    if life_xp[col].isnull().any():  # Check if the column has any null values
        life_xp[col] = life_xp[col].fillna(life_xp[col].mean())  # Fill null values in the column with the mean of non-null values

missing_values = life_xp.isnull().sum()
null_values = life_xp.isna().sum()

print("Missing values count for each column:")
print(missing_values)

print("Null value count for each column:")
print(null_values)

# Calculate the percentage of null values in each column of the DataFrame life_xp after handling both null and missing values

import matplotlib.pyplot as plt
import pandas as pd

total_rows = len(life_xp)
null_percentages = (life_xp.isnull().sum() / total_rows) * 100
missing_percentages = (life_xp.isna().sum() / total_rows) * 100

combined_percentages = [null + missing for null, missing in zip(null_percentages, missing_percentages)]

plt.figure(figsize=(12, 6))
bar_width = 0.3
index = range(len(life_xp.columns))

plt.bar(index, null_percentages, width=bar_width, color='skyblue', label='Null Values')
plt.bar([i + bar_width for i in index], missing_percentages, width=bar_width, color='salmon', label='Missing Values')
plt.bar([i + 2*bar_width for i in index], combined_percentages, width=bar_width, color='lightgreen', label='Combined')

plt.title('Rate of Null and Missing Values in Each Column')
plt.xlabel('Columns')
plt.ylabel('Percentage (%)')
plt.xticks([i + bar_width for i in index], life_xp.columns, rotation=90)
plt.legend()
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# checking for duplicates
life_xp.duplicated().sum()

"""**Selects numeric columns from the DataFrame life_xp, creates boxplots for each numeric column to detect outliers, and then displays the boxplots in a grid layout**"""

num_columns = life_xp.select_dtypes(include=np.number).columns

plt.figure(figsize=(18, 10))
sns.set(style="whitegrid")
for i, col in enumerate(num_columns, start=1):
    plt.subplot(2, int(np.ceil(len(num_columns) / 2)), i)
    sns.boxplot(y=life_xp[col], color='skyblue', flierprops=dict(marker='o', markerfacecolor='r', markersize=8))
    plt.title(f'Outliers in {col}')
    plt.ylabel(col)
plt.tight_layout()
plt.show()

"""**Calculates the lower and upper bounds for outliers using the Interquartile Range (IQR) method, then replaces outliers with the median value for each column**"""

def handle_outliers_with_iqr(data):
    # Calculate Q1 (25th percentile) and Q3 (75th percentile)
    Q1 = data.quantile(0.25)
    Q3 = data.quantile(0.75)
    # Calculate IQR (Interquartile Range)
    IQR = Q3 - Q1
    # Define the lower and upper bounds for outliers
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR

    # Replace outliers with median
    for col in data.columns:
        outliers_mask = (data[col] < lower_bound[col]) | (data[col] > upper_bound[col])
        data.loc[outliers_mask, col] = data[col].median()

# Handle outliers with IQR for numerical columns
handle_outliers_with_iqr(life_xp[num_columns])

columns_list = life_xp.columns.tolist()
print(columns_list)

"""# **DATA VISUALIZATION**

Creates a scatter plot grid to visualize the relationship between each feature and the target variable. Each subplot represents a feature, and the x-axis represents the feature values while the y-axis represents the target variable (Life expectancy).
"""

features = ['Country', 'Year', 'Status', 'Adult Mortality', 'infant deaths', 'Alcohol', 'percentage expenditure', 'Hepatitis B', 'Measles', 'BMI', 'under-five deaths', 'Polio', 'Total expenditure', 'Diphtheria', 'HIV/AIDS', 'GDP', 'Population', 'thinness  1-19 years', 'thinness 5-9 years', 'Income composition of resources', 'Schooling']
target = 'Life expectancy'

X = life_xp[features]
y = life_xp[target]

plt.figure(figsize=(18, 10))
for i, feature in enumerate(features, start=1):
    plt.subplot(5, 5, i)
    plt.scatter(X[feature], y, alpha=0.5)
    plt.xlabel(feature)
    plt.ylabel(target)
plt.tight_layout()
plt.show()

"""Creates a countplot to visualize the distribution of 'Status' (developed or developing) in the DataFrame life_xp."""

plt.figure(figsize=(8, 6))
sns.countplot(x='Status', data=life_xp, palette='Set2')
plt.title('Count of Developed and Developing Status')
plt.xlabel('Status')
plt.ylabel('Count')
plt.show()

"""Creates KDE plots for each numeric column in the DataFrame life_xp, grouped by the 'Status' variable."""

plt.figure(figsize=(30,30))

counter = 0

for col in num_columns:
    plt.subplot(5,4,counter+1)
    g = sns.kdeplot(x=col, data=life_xp, fill=True, hue="Status")
    plt.xticks(rotation=45)
    plt.title(f'Plot for {col} by Status')
    plt.xlabel(col)
    plt.ylabel('Density')
    counter += 1

plt.tight_layout()
plt.show()

"""Line plots to visualize the relationship between each feature and 'Life expectancy' in the DataFrame life_xp."""

plt.figure(figsize=(20, 20))
for i, feature in enumerate(features, start=1):
    plt.subplot(5, 5, i)
    sns.lineplot(x=feature, y='Life expectancy', data=life_xp)
    plt.xlabel(feature)
    plt.ylabel('Life expectancy')
plt.tight_layout()
plt.show()

"""Calculates the correlation matrix for numeric columns in the DataFrame life_xp and then creates a heatmap to visualize the correlations between these columns."""

# Compute correlation matrix for numeric columns
corr_matrix = life_xp.corr(numeric_only=True)

plt.figure(figsize=(18, 8))
sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', vmin=-1, vmax=1)
plt.title('Correlation Heatmap of All Columns')
plt.show()

def cleaning(data):
        # strip column names
        data = data.rename(columns = lambda x:x.strip())

        # Remove wrong values
        cols = ["infant deaths", "Measles", "under-five deaths"]
        for col in cols:
            data.loc[data[col]>1000, col] = np.nan

        return data

"""# **MODEL CREATION AND EVALUATION**"""

# tools
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler
from sklearn.impute import SimpleImputer

# models
from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor
from xgboost import XGBRegressor
from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor
from sklearn.linear_model import LinearRegression


# metrics
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score
from sklearn.metrics import confusion_matrix ,ConfusionMatrixDisplay

X = life_xp.drop(columns=["Life expectancy","Year", "Country"])  # Select predictor variables by dropping 'Life expectancy', 'Year', and 'Country' columns
y = life_xp["Life expectancy"]  # Select target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)  # Split data into training and testing sets

# Print the shapes of training and testing sets

print("The Shape of X_train is :",X_train.shape)
print("The Shape of y_train is :",y_train.shape)
print("The Shape of X_test is :",X_test.shape)
print("The Shape of y_test is :",y_test.shape)

# Categorical Columns

cat_col = [col for col in X.columns if X[col].dtype == 'object']
print(cat_col)

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

# Encode 'Status' column in training and testing sets
X_train['Status'] = encoder.fit_transform(X_train['Status'])
X_test['Status'] = encoder.transform(X_test['Status'])

# Numerical Columns

num_cols = [col for col in X.columns if X[col].dtype != 'object']
print(num_cols)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()

# Standardize numerical columns in training and testing sets
X_train[num_cols] = sc.fit_transform(X_train[num_cols])
X_test[num_cols] = sc.transform(X_test[num_cols])

# Display the first few rows of the training set predictor variables (X_train)
X_train.head()

from sklearn.ensemble import RandomForestRegressor
import matplotlib.pyplot as plt
import pandas as pd

# Initialize RandomForestRegressor model with random state 42
model = RandomForestRegressor(random_state=42)

# Fit the model on the training data
model.fit(X_train, y_train)

# Get feature importances from the trained model
feature_importance = model.feature_importances_

# Create DataFrame to store feature names and importances
feature_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': feature_importance})

# Sort the DataFrame by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot the top 20 feature importances
plt.figure(figsize=(12, 8))
bars = plt.barh(feature_importance_df['Feature'][:20], feature_importance_df['Importance'][:20], color='skyblue')

# Customize bar colors based on importance
for bar in bars:
    bar.set_alpha(0.7)
    bar.set_color(plt.cm.coolwarm(bar.get_width() / max(feature_importance_df['Importance'])))

plt.xlabel('Feature Importance')
plt.ylabel('Feature Name')
plt.title('Top 20 Feature Importances')
plt.grid(axis='x', linestyle='--', alpha=0.5)
plt.show()

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

model_name = []  # Initialize list to store model names
RMSE = []  # Initialize list to store Root Mean Squared Error (RMSE) values
R2_score = []  # Initialize list to store R-squared (R2) scores

models = [
    LinearRegression(),  # Create instance of LinearRegression model
]

# Iterate over models
for model in models:
    model.fit(X_train, y_train)  # Train the model on the training data
    prediction = model.predict(X_test)  # Make predictions on the testing data

    # Append model name, RMSE, and R2 score to respective lists
    model_name.append(model.__class__.__name__)  # Append model name
    RMSE.append(str(mean_squared_error(prediction, y_test, squared=False)))  # Append RMSE
    R2_score.append(str(r2_score(y_test, prediction) * 100) + " %")  # Append R2 score

model_life_xp = pd.DataFrame({"Model-Name": model_name, "RMSE": RMSE, "R2_Score": R2_score})  # Create DataFrame to store model results
model_life_xp = model_life_xp.set_index('Model-Name')  # Set 'Model-Name' column as index
model_life_xp.sort_values("R2_Score", ascending=False)  # Sort DataFrame by R2 score in descending order

from sklearn.ensemble import ExtraTreesRegressor

best_model = ExtraTreesRegressor(random_state=42)  # Initialize ExtraTreesRegressor model with random state 42
final_model = best_model.fit(X_train, y_train)  # Train the model on the training data

from sklearn.model_selection import cross_val_score

# Calculate R-squared (R2) scores using cross-validation for the final trained ExtraTreesRegressor model
Random_R2s = cross_val_score(final_model, X_train, y_train, scoring="r2", cv=20)

pd.Series(Random_R2s).describe()

y_hat = final_model.predict(X_test)

# Calculate Root Mean Squared Error (RMSE) between actual and predicted values
rmse = mean_squared_error(y_test, y_hat, squared=False)
print('rmse: ' + str( rmse ))

# Calculate R-squared (R2) score between actual and predicted values
r2 = r2_score(y_test, y_hat)
print("R2  : " +str(r2))

